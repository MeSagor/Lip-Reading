{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [x for x in \" অআইঈউঊঋএঐওঔকখগঘঙচছজঝঞটঠডঢণতথদধনপফবভমযরলশষসহড়ঢ়য়ৎংঃঁািীুূেৈোৌৃ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token=\"\")\n",
    "num_to_char = tf.keras.layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \\n\"\n",
    "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_num([' ','অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([bytes.decode(x) for x in num_to_char([14,  9,  3, 11, 13]).numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for recovering the ram chunk error and load and make the tensor dataset train_data,test_data and val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "\n",
    "def load_chunk(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        frames, labels = pickle.load(f)\n",
    "    return frames, labels\n",
    "\n",
    "def data_generator(file_paths):\n",
    "    for file_path in file_paths:\n",
    "        frames, labels = load_chunk(file_path)\n",
    "        yield frames, labels\n",
    "\n",
    "def create_tf_dataset_from_chunks(directory, batch_size=2):\n",
    "    file_paths = glob.glob(os.path.join(directory, '*.pkl'))\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(115, 54, 90, 1), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(55,), dtype=tf.int64),\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(file_paths),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## call the dataset function for create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_tf_dataset_from_chunks('train_chunks', batch_size=2)\n",
    "val_data = create_tf_dataset_from_chunks('val_chunks', batch_size=2)\n",
    "test_data = create_tf_dataset_from_chunks('test_chunks', batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_data.as_numpy_iterator()\n",
    "val = sample.next()\n",
    "# print(val[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = val_data.as_numpy_iterator()\n",
    "val1 = sample.next()\n",
    "# print(val[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = test_data.as_numpy_iterator()\n",
    "val2 = sample.next()\n",
    "# print(val[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0:videos, 0: 1st video out of the batch,  0: return the first frame in the video \n",
    "plt.imshow(val[0][0][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.strings.reduce_join([num_to_char(word) for word in val[1][0]])\n",
    "\n",
    "words = [num_to_char(word) for word in val[1][0]]\n",
    "joined_words = tf.strings.reduce_join(words).numpy().decode('utf-8')\n",
    "print(joined_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design the Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.as_numpy_iterator().next()[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_frames = 115\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(128, 3, input_shape=(tot_frames, 54, 90, 1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool3D((1,2,2)))\n",
    "\n",
    "model.add(Conv3D(256, 3, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool3D((1,2,2)))\n",
    "\n",
    "model.add(Conv3D(tot_frames, 3, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool3D((1,2,2)))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(Bidirectional(LSTM(256, kernel_initializer='Orthogonal', return_sequences=True)))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Bidirectional(LSTM(256, kernel_initializer='Orthogonal', return_sequences=True)))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Dense(char_to_num.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'input shape: {model.input_shape}')\n",
    "print(f'output shape: {model.output_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Training Options and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 30:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTCLoss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProduceExample(tf.keras.callbacks.Callback): \n",
    "    def __init__(self, dataset) -> None: \n",
    "        self.dataset = dataset.as_numpy_iterator()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None) -> None:\n",
    "        data = self.dataset.next()\n",
    "        yhat = self.model.predict(data[0])\n",
    "        \n",
    "        decoded = tf.keras.backend.ctc_decode(yhat, [tot_frames, tot_frames], greedy=False)[0][0].numpy()\n",
    "        for x in range(len(yhat)):           \n",
    "            print('Original:', tf.strings.reduce_join(num_to_char(data[1][x])).numpy().decode('utf-8'))\n",
    "            print('Prediction:', tf.strings.reduce_join(num_to_char(decoded[x])).numpy().decode('utf-8'))\n",
    "            print('~'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_calculation(y_true_np, y_pred_np):\n",
    "    total_chars = 0\n",
    "    correct_chars = 0\n",
    "    total_words = 0\n",
    "    correct_words = 0\n",
    "    \n",
    "    for true_row, pred_row in zip(y_true_np, y_pred_np):\n",
    "        true_row = true_row.astype(int)\n",
    "        pred_row = pred_row.astype(int)\n",
    "        \n",
    "        true_str = tf.strings.reduce_join(num_to_char(true_row)).numpy().decode('utf-8')\n",
    "        pred_str = tf.strings.reduce_join(num_to_char(pred_row)).numpy().decode('utf-8')\n",
    "        \n",
    "        total_chars += len(true_str)\n",
    "        correct_chars += sum(1 for true_char, pred_char in zip(true_str, pred_str) if true_char == pred_char)\n",
    "\n",
    "        total_words += len(true_str.split())\n",
    "        correct_words += sum(1 for true_word, pred_word in zip(true_str.split(), pred_str.split()) if true_word == pred_word)\n",
    "    \n",
    "    char_accuracy = correct_chars / total_chars if total_chars > 0 else 0\n",
    "    word_accuracy = correct_words / total_words if total_words > 0 else 0\n",
    "\n",
    "    return np.array(char_accuracy, dtype=np.float32), np.array(word_accuracy, dtype=np.float32)\n",
    "\n",
    "def word_accuracy(y_true, y_pred):\n",
    "    decoded_pred = tf.keras.backend.ctc_decode(y_pred, input_length=tf.fill([tf.shape(y_pred)[0]], tf.shape(y_pred)[1]))[0][0]\n",
    "    decoded_pred = tf.cast(decoded_pred, tf.int64)\n",
    "\n",
    "    y_true = tf.keras.backend.ctc_label_dense_to_sparse(y_true, tf.fill([tf.shape(y_true)[0]], tf.shape(y_true)[1]))\n",
    "    y_pred = tf.keras.backend.ctc_label_dense_to_sparse(decoded_pred, tf.fill([tf.shape(decoded_pred)[0]], tf.shape(decoded_pred)[1]))\n",
    "\n",
    "    y_true_dense = tf.sparse.to_dense(y_true, default_value=-1)\n",
    "    y_pred_dense = tf.sparse.to_dense(y_pred, default_value=-1)\n",
    "\n",
    "    char_accuracy, word_accuracy = tf.numpy_function(do_calculation, [y_true_dense, y_pred_dense], [tf.float32, tf.float32])\n",
    "\n",
    "    return tf.convert_to_tensor(word_accuracy, dtype=tf.float32)   #,tf.convert_to_tensor(char_accuracy, dtype=tf.float32)\n",
    "    # return word_accuracy\n",
    "\n",
    "\n",
    "\n",
    "def char_accuracy(y_true, y_pred):\n",
    "    # Decode the predictions using CTC decode\n",
    "    decoded_pred = tf.keras.backend.ctc_decode(y_pred, input_length=tf.fill([tf.shape(y_pred)[0]], tf.shape(y_pred)[1]))[0][0]\n",
    "    decoded_pred = tf.cast(decoded_pred, tf.int64)\n",
    "\n",
    "    # Removing the padding (value -1)\n",
    "    y_true = tf.keras.backend.ctc_label_dense_to_sparse(y_true, tf.fill([tf.shape(y_true)[0]], tf.shape(y_true)[1]))\n",
    "    y_pred = tf.keras.backend.ctc_label_dense_to_sparse(decoded_pred, tf.fill([tf.shape(decoded_pred)[0]], tf.shape(decoded_pred)[1]))\n",
    "    \n",
    "    # Converting sparse tensor to dense tensor\n",
    "    y_true_dense = tf.sparse.to_dense(y_true, default_value=-1)\n",
    "    y_pred_dense = tf.sparse.to_dense(y_pred, default_value=-1)\n",
    "    \n",
    "    # Use tf.numpy_function to print and return the numpy arrays\n",
    "    char_accuracy, _ = tf.numpy_function(do_calculation, [y_true_dense, y_pred_dense], [tf.float32, tf.float32])\n",
    "    \n",
    "    return tf.convert_to_tensor(char_accuracy, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsHistory(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_file):\n",
    "        super(MetricsHistory, self).__init__()\n",
    "        self.log_file = log_file\n",
    "        os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(f'Epoch {epoch + 1:02d}: '\n",
    "                    f'Train Loss: {logs[\"loss\"]:<10.4f} '\n",
    "                    f'Train char Accuracy: {logs[\"char_accuracy\"]:<10.4f} '\n",
    "                    f'Train word accuracy: {logs[\"word_accuracy\"]:<10.4f} '\n",
    "                    f'Val Loss: {logs[\"val_loss\"]:<10.4f} '\n",
    "                    f'Val char accuracy: {logs[\"val_char_accuracy\"]:<10.4f} '\n",
    "                    f'Val word accuracy: {logs[\"val_word_accuracy\"]:<10.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join('bangla_model','checkpoint'),\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    "    ) \n",
    "\n",
    "schedule_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "example_callback = ProduceExample(val_data)\n",
    "\n",
    "# Initialize the custom callback\n",
    "log_file = './metrics.txt'\n",
    "metrics_history = MetricsHistory(log_file=log_file)\n",
    "\n",
    "backup_callback = tf.keras.callbacks.BackupAndRestore(\n",
    "    os.path.join('bangla_training_backup'),\n",
    "    save_freq='epoch',\n",
    "    delete_checkpoint=True\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15\n",
    ")\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss=CTCLoss, metrics=[word_accuracy, char_accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, validation_data=val_data, epochs=150, callbacks=[\n",
    "                    backup_callback, checkpoint_callback, schedule_callback, early_stopping, example_callback, metrics_history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrices plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read data from file\n",
    "with open(\"metrics.txt\", \"r\") as file:\n",
    "   data = file.readlines()\n",
    "\n",
    "# Parse the data\n",
    "epochs = []\n",
    "train_loss = []\n",
    "train_char_accuracy = []\n",
    "train_word_accuracy = []\n",
    "val_loss = []\n",
    "val_char_accuracy = []\n",
    "val_word_accuracy = []\n",
    "\n",
    "\n",
    "for line in data:\n",
    "   parts = line.split()\n",
    "   epochs.append(int(parts[1][:-1]))  # Extracting epoch number\n",
    "   train_loss.append(float(parts[4]))\n",
    "   train_char_accuracy.append(float(parts[8]))\n",
    "   train_word_accuracy.append(float(parts[12]))\n",
    "   val_loss.append(float(parts[15]))\n",
    "   val_char_accuracy.append(float(parts[19]))\n",
    "   val_word_accuracy.append(float(parts[23]))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(2, 2, (1, 2))\n",
    "plt.plot(epochs, train_loss, '-o', label='Train Loss')\n",
    "plt.plot(epochs, val_loss, label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(2, 2, (3, 4))\n",
    "plt.plot(epochs, train_char_accuracy, '-o', label='Train Char Accuracy')\n",
    "plt.plot(epochs, val_char_accuracy, '-o', label='Val Char Accuracy')\n",
    "plt.plot(epochs, train_word_accuracy, '-o', label='Train Word Accuracy')\n",
    "plt.plot(epochs, val_word_accuracy, '-o', label='Val Word Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Save plot to file\n",
    "plt.savefig('metrics_plot.png')\n",
    "# Show a message indicating that the plot is saved\n",
    "print(\"Plot saved as 'metrics_plot.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for windows\n",
    "# model.load_weights('.\\\\models\\\\checkpoint')\n",
    "model.load_weights('./bangla_model/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_single = val_data.as_numpy_iterator()\n",
    "\n",
    "for _ in range(20):\n",
    "    data = val_single.next()\n",
    "    yhat = model.predict(data[0])\n",
    "    \n",
    "    decoded = tf.keras.backend.ctc_decode(yhat, [tot_frames, tot_frames], greedy=False)[0][0].numpy()\n",
    "    for x in range(len(yhat)):           \n",
    "        print('Original:', tf.strings.reduce_join(num_to_char(data[1][x])).numpy().decode('utf-8'))\n",
    "        print('Prediction:', tf.strings.reduce_join(num_to_char(decoded[x])).numpy().decode('utf-8'))\n",
    "        print('~'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_single = test_data.as_numpy_iterator()\n",
    "\n",
    "for _ in range(20):\n",
    "    data = test_single.next()\n",
    "    yhat = model.predict(data[0])\n",
    "    decoded = tf.keras.backend.ctc_decode(yhat, [tot_frames, tot_frames], greedy=False)[0][0].numpy()\n",
    "    for x in range(len(yhat)):\n",
    "        print('Original:', tf.strings.reduce_join(num_to_char(data[1][x])).numpy().decode('utf-8'))\n",
    "        print('Prediction:', tf.strings.reduce_join(num_to_char(decoded[x])).numpy().decode('utf-8'))\n",
    "        print('~'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltp_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
